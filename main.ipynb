{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dict(filename):\n",
    "    # {\"word\" : (word_id, count)}\n",
    "    word_count_dict = {\"UNK\" : 100}\n",
    "    \n",
    "    f = open(filename, 'r')\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        tokens = line.split(' ')\n",
    "        \n",
    "        for index, token in enumerate(tokens):\n",
    "            if len(token) == 0:\n",
    "                continue\n",
    "            if \"\\\\*\"  in token:\n",
    "                token = token.replace(\"\\\\*\", \"*\")\n",
    "            if \"\\\\/\"  in token:\n",
    "                token = token.replace(\"\\\\/\", \"#\")\n",
    "                \n",
    "            word, _ = token.split('/')\n",
    "            \n",
    "            word = word.lower()\n",
    "\n",
    "            if word not in word_count_dict.keys():\n",
    "                word_count_dict[word] = 1\n",
    "            else:\n",
    "                word_count_dict[word] += 1\n",
    "    \n",
    "    word_dict = {}\n",
    "    word_id = 0\n",
    "    for word, count in word_count_dict.items():\n",
    "        if count <= 5:\n",
    "            continue\n",
    "        else:\n",
    "            word_dict[word] = word_id\n",
    "            word_id += 1\n",
    "    \n",
    "    return word_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(word, index, len_sentence, word_id, prev_word_id):\n",
    "\n",
    "#     prev_one_hot = np.array([0 for i in range(dim_words)])\n",
    "\n",
    "    one_hot = np.identity(dim_words)[word_id].tolist()\n",
    "\n",
    "    feature_dict = {\n",
    "        'is_first_capital': int(word[0].isupper()),\n",
    "        'is_first_word': int(index == 0),\n",
    "        'is_last_word': int(index == len_sentence - 1),\n",
    "        'is_numeric' : int(word.isdigit()),\n",
    "        'is_all_capital': int(word.upper() == word)\n",
    "    }\n",
    "\n",
    "    feature = one_hot + list(feature_dict.values())\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, labels, word_ids, words, pos_tags, tokens):\n",
    "        self.labels = labels\n",
    "        self.word_ids = word_ids\n",
    "        self.words = words\n",
    "        self.pos_tags = pos_tags\n",
    "        self.tokens = tokens\n",
    "        self.features = None\n",
    "        \n",
    "    def show(self):\n",
    "        print(self.tokens)\n",
    "        print(self.words)\n",
    "        print(self.pos_tags)\n",
    "        print(self.labels)\n",
    "        print(self.word_ids)\n",
    "        print(features.shape)\n",
    "        \n",
    "    def extract_features(self, dim_words):\n",
    "        \n",
    "        if self.features is not None:\n",
    "            return self.features\n",
    "        \n",
    "        prev_one_hot = np.array([0 for i in range(dim_words)])\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        one_hot_table = np.identity(dim_words)[self.word_ids]\n",
    "        \n",
    "        for index, word in enumerate(self.words):\n",
    "#             word_id = self.word_ids[index]\n",
    "            one_hot = one_hot_table[index].tolist()\n",
    "            \n",
    "            feature_dict = {\n",
    "                'is_first_capital': int(word[0].isupper()),\n",
    "                'is_first_word': int(index == 0),\n",
    "                'is_last_word': int(index == len(self.words) - 1),\n",
    "                'is_numeric' : int(word.isdigit()),\n",
    "                'is_all_capital': int(word.upper() == word)\n",
    "            }\n",
    "            \n",
    "            feature = one_hot + list(feature_dict.values())\n",
    "            features.append(feature)\n",
    "        \n",
    "        features = np.array(features)\n",
    "        self.features = features\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(10)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples(filename):\n",
    "    f = open(filename, 'r')\n",
    "    data = []\n",
    "    \n",
    "    for line in tqdm(f.readlines()):\n",
    "        line = line.strip()\n",
    "        \n",
    "#         line = \"##/## \" + line + \" $$/$$\"\n",
    "        tokens = line.split(' ')\n",
    "\n",
    "        word_ids = []\n",
    "        labels = []\n",
    "        words = []\n",
    "        pos_tags = []\n",
    "        features = []\n",
    "        \n",
    "        len_sentence = len(tokens)\n",
    "        \n",
    "        \n",
    "        prev_word_id = -1\n",
    "        \n",
    "        for index, token in enumerate(tokens):\n",
    "            if len(token) == 0:\n",
    "                continue\n",
    "            if \"\\\\*\"  in token:\n",
    "                token = token.replace(\"\\\\*\", \"*\")\n",
    "            if \"\\\\/\"  in token:\n",
    "                token = token.replace(\"\\\\/\", \"#\")\n",
    "                \n",
    "            word, pos = token.split('/')\n",
    "            words.append(word)\n",
    "            \n",
    "            pos_tags.append(pos)\n",
    "\n",
    "            if pos not in pos_dict.keys():\n",
    "                pos_dict[pos] = len(pos_dict)\n",
    "            \n",
    "            pos_id = pos_dict[pos]\n",
    "            labels.append(pos_id)\n",
    "            \n",
    "            lower_word = word.lower()\n",
    "            if lower_word not in word_dict.keys():\n",
    "                lower_word = \"UNK\"\n",
    "\n",
    "            word_id = word_dict[lower_word]\n",
    "            word_ids.append(word_id)\n",
    "            \n",
    "#             feature_vec = extract_features(word, index, len_sentence, word_id, prev_word_id)\n",
    "#             features.append(feature_vec)\n",
    "            \n",
    "            prev_word_id = word_id\n",
    "\n",
    "\n",
    "        sentence = Sentence(labels, word_ids, words, pos_tags, tokens)\n",
    "        data.append(sentence)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence, features):\n",
    "\n",
    "#     predicted_pos_ids = [-1] # 開始記号分\n",
    "\n",
    "    predicted_pos_ids = [-1]\n",
    "\n",
    "    for ind, word_id in enumerate(sentence.word_ids):\n",
    "#         if word_id == 0 or word_id == 1:\n",
    "#             continue\n",
    "\n",
    "        feature_vec = features[ind]        \n",
    "        max_score = -np.inf\n",
    "        max_i = -1\n",
    "\n",
    "        for i in range(len(weight_vectors)):\n",
    "            score = np.dot(weight_vectors[i], feature_vec)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "#         max_scores.append(max_score)\n",
    "        predicted_pos_ids.append(max_i)\n",
    "    \n",
    "#     predicted_pos_ids.append(-1) # 終了記号分\n",
    "#     features.append([-1, -1, -1])\n",
    "    return predicted_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##→開始記号, $$→終了記号\n",
    "\n",
    "# pos_dict = {\"##\": 0, \"$$\": 1}\n",
    "# feature_dict = {\"##\": 0, \"$$\": 1}\n",
    "\n",
    "pos_dict = {}\n",
    "word_dict = create_word_dict(\"data/train.pos\")\n",
    "dim_words =  len(word_dict)\n",
    "\n",
    "weight_vectors = []\n",
    "u_vectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38219/38219 [00:02<00:00, 17347.69it/s]\n",
      " 39%|███▉      | 2172/5527 [00:00<00:00, 21718.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5527/5527 [00:00<00:00, 19704.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "9543\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "valid_data = []\n",
    "training_data = read_samples(\"data/train.pos\")\n",
    "print(len(pos_dict))\n",
    "print(len(word_dict))\n",
    "valid_data = read_samples(\"data/val.pos\")\n",
    "print(len(pos_dict))\n",
    "print(len(word_dict))\n",
    "\n",
    "dim_words = len(word_dict)\n",
    "\n",
    "# initialization\n",
    "fe = training_data[0].extract_features(dim_words)\n",
    "feature_num = fe.shape[1]\n",
    "weight_vectors = np.random.uniform(low=-0.08, high=0.08, size=(len(pos_dict), feature_num)).astype('float32')\n",
    "u_vectors = np.zeros(shape=(len(pos_dict), feature_num)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9543"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = training_data[0].word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.identity(dim_words)[a].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = training_data[0].extract_features(dim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38219/38219 [28:59<00:00, 21.97it/s] \n",
      "  0%|          | 3/38219 [00:00<23:47, 26.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 num_updates =  846775 accuracy =  7.235270918691528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 35554/38219 [17:32<02:01, 21.94it/s]  "
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "for epoch in range(100):\n",
    "    num_updates = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for j in tqdm(range(len(training_data))):\n",
    "        r = random.randint(0, len(training_data) - 1)\n",
    "        sentence = training_data[r]\n",
    "        \n",
    "        features = sentence.extract_features(dim_words)\n",
    "        predicted_pos_ids = classify(sentence, features)\n",
    "\n",
    "        for i in range(len(sentence.labels)):\n",
    "            label, y, feature_vec = sentence.labels[i],predicted_pos_ids[i], features[i]\n",
    "\n",
    "#             if label in [0, 1]:\n",
    "#                 continue\n",
    "            \n",
    "            total += 1\n",
    "            \n",
    "            # 重みの更新    \n",
    "            if label == y:\n",
    "                correct += 1\n",
    "                continue\n",
    "\n",
    "            weight_vectors[label] += feature_vec\n",
    "            weight_vectors[y] -= feature_vec\n",
    "            num_updates += 1\n",
    "    print(epoch, \"num_updates = \", num_updates, \"accuracy = \", correct / total * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for sentence in valid_data:\n",
    "    predicted_pos_ids, features = classify(sentence)\n",
    "    for i in range(len(sentence.labels)):\n",
    "        label, y, _ = sentence.labels[i], predicted_pos_ids[i], features[i]\n",
    "#         if label in [0, 1]:\n",
    "#             continue\n",
    "        if (y == label):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(\"accuracy = \", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos\n",
    "# 今のままだとword_idをそのままfeatureの値として扱っているので順番によって大小が影響されてします．look up tableにembeddingしたほうが良さそう\n",
    "# featureの設計\n",
    "\n",
    "# 重みは0じゃなくてrandomのほうが良さそう\n",
    "# trainの中に出現していない語はUNKNOWN\n",
    "# • 小文字化:全ての単語を小文字に変換\n",
    "# • 未知語:学習データで頻度1以下の単語、テスト データで未知の単語を “UNK” に変換"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
